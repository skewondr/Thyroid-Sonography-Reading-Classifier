{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT finetuning runner.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "#from transformers import BertAdam\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        sample 하나 당 하나의 InputExample. 다음과 같은 argument가 전달 됨.  \n",
    "            guid: 하나의 example에 대한 고유 번호.\n",
    "            text_a: 토큰화 하기 전 첫번째 문장\n",
    "            text_b: 토큰화 하기 전 두번째 문장\n",
    "            label: (문자열) example의 라벨. \n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        \"\"\"\n",
    "        Constructs a InputExample.\n",
    "        sample 하나 당 하나의 InputFeatures. 다음과 같은 argument가 전달 됨.  \n",
    "            input_ids: 토큰화된 문장을 지정 인덱스로 변환한 것\n",
    "            input_mask: 패딩된 토큰에 대해서는 0으로 둠. \n",
    "            segment_ids: 첫문장(0)인지 두번째 문장(1)인지 구분.\n",
    "            label_id: example의 라벨. 여기서는 0 또는 1. \n",
    "        \"\"\"\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\n",
    "            파일에서 한 줄 씩 읽어 lines 리스트에 저장합니다. \n",
    "        \"\"\"\n",
    "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            next(reader, None) #header 제외하기 \n",
    "            lines = []\n",
    "            for line in reader: \n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"이 코드는 thyroid 데이터를 사용해 학습하기 때문에 ThyProcess만을 다룹니다 \"\"\"\n",
    "\n",
    "class ThyProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Thyroid Sonography data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\"] # 문서 클래스가 3종류면 여기를 0, 1, 2로 바꾸면 됨. 순서 꼭 맞추기\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\n",
    "           각 줄에서 guid, text_a, text_b, label을 추출하여 InputExample을 통해 데이터 재정렬.    \n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i) #train/dev + index \n",
    "            text_a = line[0]\n",
    "            text_b = None\n",
    "            label = line[1]\n",
    "            #print(\"label: \", label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "    두 문장이 들어오고 max_length가 주어졌을 때, 두 문장의 길이를 공평하게 맞추어 truncate하고자 함. \n",
    "    1) 합친 문장이 max_length보다 작을 때 -> 종료 \n",
    "    2) 첫 문장이 두번째 문장보다 길때 -> 첫 문장에서 맨 뒤 토큰 하나 없애기 \n",
    "    3) 두번째 문장이 첫 문장보다 길때 -> 두번째 문장에서 맨 뒤 토큰 하나 없애기 \n",
    "    -> max_length에 도달할 때까지 무한 반복. \n",
    "    \"\"\"\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop() #pop() 안에 index argument가 없으면 맨 뒤의 것을 없앤다. \n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"InputExample >> InputFeatures로 변환해주는 기능을 합니다.\"\"\"\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    # label_list: 0,1 label 의 종류\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    #print(\"label_map: \", label_map) \n",
    "    # label_map:  {'0': 0, '1': 1, '2':2}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        \n",
    "#         print(\"tokens_a: \")\n",
    "#         print(tokens_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b: #만약 text_b가 존재한다면 토큰화 해준다. \n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            \n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            #첫문장과 끝문장의 길이를 max_seq_length에 적절히 맞추어 붙인다. \n",
    "            \n",
    "#             print(\"tokens_b: \", tokens_b)\n",
    "            \n",
    "            \n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        \n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            #만약 첫문장이 special tokens를 고려했을 때 max_seq_length보다 더 크다면?\n",
    "            # 첫문장 뒷부분을 자른다. \n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        \n",
    "        #special tokens과 max_seq_length를 고려하여 길이 조절된 토큰화된 문장에 대해 special tokens 추가 \n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b: #만약 두번째 문장이 존재한다면 \n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        \n",
    "#         print(\"tokens: \", tokens)\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "#         print(\"input_ids: \", input_ids)\n",
    "        \n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        #input-mask 0인 부분은 attention score을 적용하지 않도록 함. \n",
    "        \"\"\"positional embedding은 bert내부에 있거나 안쓴 가능성 있음. \"\"\"\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        # 세 가지 임베딩 벡터의 길이가 max_seq_length와 맞는지 확인 \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        \n",
    "        #5개의 examples에 대해서 처리된 데이터 출력하기 \n",
    "#         if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"guid: %s\" % (example.guid))\n",
    "#             logger.info(\"tokens: %s\" % \" \".join(\n",
    "#                     [str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                     \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "#             logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "            # print(\"label: \", example.label, label_id)\n",
    "        \n",
    "        #생성된 features는 features리스트에 저장되어 리턴됨. \n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "#     \"\"\"InputExample >> InputFeatures로 변환해주는 기능을 합니다.\"\"\"\n",
    "#     \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "#     # label_list: 0,1 label 의 종류\n",
    "#     label_map = {label : i for i, label in enumerate(label_list)}\n",
    "#     #print(\"label_map: \", label_map) \n",
    "#     # label_map:  {'0': 0, '1': 1, '2':2}\n",
    "\n",
    "#     features = []\n",
    "#     for (ex_index, example) in enumerate(examples):\n",
    "#         \"\"\"첫문장 a를 white_space단위로 토큰화\"\"\"\n",
    "#         tokens_a = tokenizer.tokenize(example.text_a)\n",
    "#          \"\"\"첫문장 b는 없으므로 생략\"\"\"\n",
    "#         tokens_b = None\n",
    "#         \"\"\"문장 길이 최대값에 맞게 문장을 자르기\"\"\"\n",
    "#         # Account for [CLS] and [SEP] with \"- 2\"\n",
    "#         #만약 첫문장이 special tokens를 고려했을 때 max_seq_length보다 더 크다면?\n",
    "#         # 첫문장 뒷부분을 자른다. \n",
    "#         if len(tokens_a) > max_seq_length - 2:\n",
    "#             tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "                \n",
    "#         \"\"\"token embedding 생성\"\"\"\n",
    "#         tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "#         input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "#         \"\"\"segment embedding 생성\"\"\"\n",
    "#         segment_ids = [0] * len(tokens) #cls, sep 토큰까지 모두 0으로\n",
    "\n",
    "#         # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "#         # tokens are attended to.\n",
    "#         input_mask = [1] * len(input_ids)\n",
    "\n",
    "#         # Zero-pad up to the sequence length.\n",
    "#         padding = [0] * (max_seq_length - len(input_ids))\n",
    "#         input_ids += padding\n",
    "#         input_mask += padding\n",
    "#         segment_ids += padding\n",
    "\n",
    "#         # 세 가지 임베딩 벡터의 길이가 max_seq_length와 맞는지 확인 \n",
    "#         assert len(input_ids) == max_seq_length\n",
    "#         assert len(input_mask) == max_seq_length\n",
    "#         assert len(segment_ids) == max_seq_length\n",
    "\n",
    "#         label_id = label_map[example.label]\n",
    "        \n",
    "#         #5개의 examples에 대해서 처리된 데이터 출력하기 \n",
    "# #         if ex_index < 5:\n",
    "# #             logger.info(\"*** Example ***\")\n",
    "# #             logger.info(\"guid: %s\" % (example.guid))\n",
    "# #             logger.info(\"tokens: %s\" % \" \".join(\n",
    "# #                     [str(x) for x in tokens]))\n",
    "# #             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "# #             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "# #             logger.info(\n",
    "# #                     \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "# #             logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "#             # print(\"label: \", example.label, label_id)\n",
    "        \n",
    "#         #생성된 features는 features리스트에 저장되어 리턴됨. \n",
    "#         features.append(\n",
    "#                 InputFeatures(input_ids=input_ids,\n",
    "#                               input_mask=input_mask,\n",
    "#                               segment_ids=segment_ids,\n",
    "#                               label_id=label_id))\n",
    "#     return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_func(args, device, processor, num_labels, label_list, tokenizer, n_gpu):\n",
    "    \n",
    "    args.bert_model = args.bert_models[args.case]\n",
    "    args.data_dir = args.data_dirs[args.case]\n",
    "    args.output_dir = args.output_dirs[args.case]\n",
    "    \n",
    "#     print(\"args.bert_model_type:\",args.bert_model_type)\n",
    "#     print(\"args.output_dir:\",args.output_dir)\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \"\"\"데이터 파일 경로를 입력하고 InputExample에 대한 리턴값을 받는다.\"\"\"\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    num_train_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "    # Prepare model\n",
    "    # PYTORCH_PRETRAINED_BERT_CACHE: /home/yoonjin/.cache/torch/transformers\n",
    "    # 여러 gpu 를 쓰면 local_rank 가 -1이 아니게 되는데, 각 gpu 의 번호를 의미할 가능성이 있음\n",
    "    # 이때 각 gpu 에서 쓰는 모델을 캐싱(잠시 저장) 해두는데, 그 경로를 의미하는 듯 함\n",
    "    \"\"\"from_pretrained method\n",
    "    the model is set in evaluation mode by default using model.eval().\n",
    "    (Dropout modules are deactivated)\"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(args.bert_model_type,\n",
    "              cache_dir=PYTORCH_PRETRAINED_BERT_CACHE+\"/\"+'distributed_{}'.format(args.local_rank),\n",
    "              num_labels = num_labels)\n",
    "    \n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        model = DDP(model)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    # Prepare optimizer\n",
    "    # Separate the `weight` parameters from the `bias` parameters. \n",
    "    # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
    "    # - For the `bias` parameters, the 'weight_decay_rate' is 0.0.\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "    t_total = num_train_steps\n",
    "    if args.local_rank != -1:\n",
    "        # 가지고 있는 gpu 개수만큼 나눠줌(get_word_size)\n",
    "        t_total = t_total // torch.distributed.get_world_size()\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FusedAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                              lr=args.learning_rate,\n",
    "                              bias_correction=False,\n",
    "                              max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0:\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                lr=args.learning_rate)\n",
    "        warmup_steps = t_total*args.warmup_proportion # 초반 step 을 작은 learning rate 로 학습\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "\n",
    "    # 한 줄을 features 로 바꾼 형태\n",
    "    # 토큰의 아이디, mask 여부(전부1), segment id, label id 가 들어있음\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "#         print(\"all_label_ids: \", all_label_ids.size())\n",
    "#         print(\"all_label_ids: \", all_label_ids)\n",
    "\n",
    "    # 따로 빼낸 각 줄의 각 feature 를 다시 TensorDataset 으로 묶어줌\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if args.local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    # epoch 의 eval set 에서의 성능\n",
    "    best_eval_accuracy = -1\n",
    "    \n",
    "    # 학습 루프\n",
    "    model.train()\n",
    "    for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"): # EPOCH\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_examples, epoch_steps = 0, 0\n",
    "        \n",
    "        # 각 epoch 루프\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # 4개 feature 묶음 * batchsize = batch 을 풀어서 루프를 돌린다\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "#               #print(\"label_ids: \", label_ids)\n",
    "\n",
    "            # 한줄로 forward\n",
    "            # loss 의 0: loss\n",
    "            # loss 의 1: logits\n",
    "            loss = model(input_ids=input_ids, token_type_ids=segment_ids, \n",
    "                         attention_mask=input_mask, labels=label_ids)\n",
    "            #print(\"logits: \", loss[1])\n",
    "            \n",
    "            loss = loss[0]\n",
    "            \n",
    "            #print(\"loss: \", loss)\n",
    "\n",
    "\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                # 미분. 매개변수에 대한 손실의 변화도를 계산합니다. \n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_examples += input_ids.size(0)\n",
    "            \n",
    "            nb_tr_steps += 1\n",
    "            epoch_steps += 1\n",
    "            \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args.learning_rate * warmup_linear(global_step/t_total, args.warmup_proportion)\n",
    "                \n",
    "                \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                \n",
    "                optimizer.step() # 미분한것을 가중치에서 빼줌. 매개변수가 갱신됩니다. \n",
    "                lr_scheduler.step() # 학습 스텝 개수를 세어주는 역할. warmup 에서 learning rate 를 계산해주는 역할\n",
    "                global_step += 1\n",
    "                optimizer.zero_grad() # 갱신할 변수들에 대한 변화도를 0으로 만듭니다. \n",
    "                                      # 그 이유는 기본적으로 .backward()를 호출할 때마다 \n",
    "                                      # 변화도가 버퍼(buffer)에 누적되기 때문입니다. \n",
    "                    \n",
    "#                 result = {'global_step': global_step,\n",
    "#                           'loss': tr_loss/nb_tr_steps}\n",
    "#                 output_eval_file = os.path.join(args.output_dir, \"train_results.txt\")\n",
    "#                 with open(output_eval_file, \"a\") as writer:\n",
    "#                     logger.info(\"***** Train results *****\")\n",
    "#                     for key in sorted(result.keys()):\n",
    "#                         #logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                         writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "            \n",
    "    \n",
    "        # 한번의 epoch 이 끝났을 때\n",
    "        # Estimation on train set \n",
    "        print(\"epoch \", epoch, \" 결과 출력\")\n",
    "        result_epoch = {'epoch': epoch, 'loss': epoch_loss/epoch_steps}\n",
    "        output_eval_file_epoch = os.path.join(args.output_dir, \"train_results_epoch.txt\")\n",
    "        with open(output_eval_file_epoch, \"a\") as writer:\n",
    "            logger.info(\"***** Train results *****\")\n",
    "            for key in sorted(result_epoch.keys()):\n",
    "                #logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result_epoch[key])))\n",
    "        \n",
    "        # Estimation on Evaluation set\n",
    "        if args.do_eval==True:\n",
    "            model.eval()\n",
    "            eval_loss, eval_accuracy = do_eval_func(args, device, processor, \n",
    "                                                    num_labels, label_list, tokenizer, n_gpu, \n",
    "                                                    model=model, epoch=epoch)\n",
    "            \n",
    "            if eval_accuracy > best_eval_accuracy:\n",
    "                # Save a trained model\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "                output_model_file = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "                best_eval_accuracy = eval_accuracy\n",
    "            model.train()\n",
    "        \n",
    "        \n",
    "\n",
    "#     # Save a trained model\n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "#     output_model_file = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
    "#     torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "#     ## test code\n",
    "#     print(\"train:: output_model_file: \", output_model_file)\n",
    "#     #output_model_file = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
    "#     output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin\")\n",
    "#     model_state_dict = torch.load(output_model_file)\n",
    "#     model = BertForSequenceClassification.from_pretrained(args.bert_model_type, state_dict=model_state_dict, num_labels = num_labels)\n",
    "#     model.to(device)\n",
    "#     print(\"로드 정상적으로 됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " ######################### Evaluation ######################### \n",
    "    # Load a trained model that you have fine-tuned\n",
    "    # 평가를 위해 fine-tuned model 불러오기 \n",
    "    \n",
    "def do_eval_func(args, device, processor, num_labels, label_list, tokenizer, n_gpu, model=None, epoch=0):\n",
    "    \n",
    "    args.bert_model = args.bert_models[args.case]\n",
    "    args.data_dir = args.data_dirs[args.case]\n",
    "    args.output_dir = args.output_dirs[args.case]\n",
    "#     print(\"args.bert_model_type:\",args.bert_model_type)\n",
    "#     print(\"args.bert_model:\",args.bert_model)\n",
    "#     print(\"args.output_dir:\",args.output_dir)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    if model==None:\n",
    "        model_dir = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
    "    #     print(\"eval:: model_dir: \", model_dir) #./models/Thyroid/pytorch_model.bin\n",
    "        model_state_dict = torch.load(model_dir)\n",
    "        model = BertForSequenceClassification.from_pretrained(args.bert_model_type, \n",
    "                                                              state_dict=model_state_dict, \n",
    "                                                              num_labels = num_labels)\n",
    "        model.to(device)\n",
    "    \n",
    "    # prediction\n",
    "    head = \"class0\\tclass1\\tclass2\\tlabel\\n\"\n",
    "    prediction_arr = []\n",
    "\n",
    "    if args.local_rank == -1 or torch.distributed.get_rank() == 0: #사용 GPU가 하나일 때 \n",
    "        \"\"\"eval 데이터에 대한 example >> features 생성\"\"\"\n",
    "        eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        # evaluation batch 루프. \n",
    "        # 4개 feature 묶음 * batchsize = batch 을 풀어서 루프를 돌린다\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "#                 tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                 logits = model(input_ids, segment_ids, input_mask)\n",
    "                tmp_eval_loss = model(input_ids=input_ids, token_type_ids=segment_ids, \n",
    "                             attention_mask=input_mask, labels=label_ids)\n",
    "                logits = tmp_eval_loss[1]\n",
    "                tmp_eval_loss = tmp_eval_loss[0]\n",
    "                 \n",
    "#                 print(\"logits.size(): \", logits.size())\n",
    "#                 print(\"label_ids: \", label_ids)\n",
    "                \n",
    "                # logits.size()[0] = 4 이하\n",
    "                for b in range(logits.size()[0]):\n",
    "                    row_result = []\n",
    "                    row_result.append(str(logits[b][0].item()))\n",
    "                    row_result.append(str(logits[b][1].item()))\n",
    "                    row_result.append(str(logits[b][2].item()))\n",
    "                    row_result.append(str(label_ids[b].item()))\n",
    "                    \n",
    "                    row_result = \"\\t\".join(row_result)\n",
    "#                     print(row_result)\n",
    "                    prediction_arr.append(row_result)\n",
    "                \n",
    "                \n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "        \n",
    "        result = {'epoch': epoch,\n",
    "                  'eval_loss': eval_loss,\n",
    "                  'eval_accuracy': eval_accuracy}\n",
    "        \n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                #logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        \n",
    "        # logits 를 출력\n",
    "        output_txt = \"\\n\".join(prediction_arr)\n",
    "        output_txt = head + output_txt\n",
    "\n",
    "        import codecs\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_logits.txt\")\n",
    "        file = codecs.open(output_eval_file, \"w\", \"utf-8\")\n",
    "        file.write(output_txt)\n",
    "        file.close()\n",
    "        \n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# 수정 참고 코드 : https://www.kaggle.com/qhd0081/crime-classification-bert-k-fold\n",
    "#batch size 4에서 한 sample씩 들어옴. \n",
    "def saving(out, labels,test_preds, test_y): \n",
    "    outputs = np.argmax(out)\n",
    "    test_preds.append(outputs)\n",
    "    test_y.append(labels)\n",
    "    return test_preds, test_y\n",
    "\n",
    "def evaluate(prediction, target):\n",
    "    print(\"prediction:\",prediction)\n",
    "    print(\"target:\",target)\n",
    "    report = classification_report(target, prediction)\n",
    "    print(report)\n",
    "    return report\n",
    "\n",
    "def do_test_func(args, device, processor, num_labels, label_list, tokenizer, n_gpu):\n",
    "    \n",
    "    args.bert_model = args.bert_models[args.case]\n",
    "    args.data_dir = args.data_dirs[args.case]\n",
    "    args.output_dir = args.output_dirs[args.case]\n",
    "#     print(\"args.bert_model_type:\",args.bert_model_type)\n",
    "#     print(\"args.bert_model:\",args.bert_model)\n",
    "#     print(\"args.output_dir:\",args.output_dir)\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    model_dir = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
    "    model_state_dict = torch.load(model_dir)\n",
    "    model = BertForSequenceClassification.from_pretrained(args.bert_model_type, \n",
    "                                                          state_dict=model_state_dict, \n",
    "                                                          num_labels = num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    # prediction\n",
    "    head = \"class0\\tclass1\\tclass2\\tlabel\\n\"\n",
    "    prediction_arr = []\n",
    "\n",
    "    if args.local_rank == -1 or torch.distributed.get_rank() == 0: #사용 GPU가 하나일 때 \n",
    "        \"\"\"eval 데이터에 대한 example >> features 생성\"\"\"\n",
    "        eval_examples = processor.get_test_examples(args.data_dir)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running test *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        # test batch 루프. \n",
    "        # 4개 feature 묶음 * batchsize = batch 을 풀어서 루프를 돌린다\n",
    "        test_preds = []\n",
    "        test_y=[]\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "#                 tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                 logits = model(input_ids, segment_ids, input_mask)\n",
    "                tmp_eval_loss = model(input_ids=input_ids, token_type_ids=segment_ids, \n",
    "                             attention_mask=input_mask, labels=label_ids)\n",
    "                logits = tmp_eval_loss[1]\n",
    "                tmp_eval_loss = tmp_eval_loss[0]\n",
    "                \n",
    "                # logits.size()[0] = 4 이하\n",
    "                \n",
    "                for b in range(logits.size()[0]):\n",
    "                    row_result = []\n",
    "                    row_result.append(str(logits[b][0].item()))\n",
    "                    row_result.append(str(logits[b][1].item()))\n",
    "                    row_result.append(str(logits[b][2].item()))\n",
    "                    print(\"row_result:\",np.array(row_result),\"label:\", label_ids[b].item())\n",
    "                    test_preds, test_y = saving(np.array(row_result), label_ids[b].item(), test_preds, test_y)\n",
    "                    \n",
    "                    row_result.append(str(label_ids[b].item()))\n",
    "                    \n",
    "                    row_result = \"\\t\".join(row_result)\n",
    "#                     print(row_result)\n",
    "                    prediction_arr.append(row_result)\n",
    "            \n",
    "                \n",
    "                \n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "        report = evaluate(test_preds, test_y)\n",
    "        \n",
    "        result = {'test_loss': eval_loss,\n",
    "                      'test_accuracy': eval_accuracy}\n",
    "        \n",
    "        output_eval_file = os.path.join(args.output_dir, \"test_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                #logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        \n",
    "        # logits 를 출력\n",
    "        output_txt = \"\\n\".join(prediction_arr)\n",
    "        output_txt = head + output_txt\n",
    "\n",
    "        import codecs\n",
    "        output_eval_file = os.path.join(args.output_dir, \"test_logits.txt\")\n",
    "        file = codecs.open(output_eval_file, \"w\", \"utf-8\")\n",
    "        file.write(output_txt)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    ## Required parameters\n",
    "    case, \n",
    "    data_dirs, # 입력데이터 경로\n",
    "    bert_model_type, # 모델 타입\n",
    "    bert_models, # 모델의 경로(로컬)\n",
    "    task_name, # Glue task name\n",
    "    output_dirs, # 출력경로\n",
    "    \n",
    "    ## Other parameters\n",
    "    max_seq_length=512, #첫번째 문장 + 두번째 문장 + special tokens 다 합칠 때 최대 허용 토큰 개수  \n",
    "    do_train=True, \n",
    "    do_eval=False,\n",
    "    do_test=False, \n",
    "    do_lower_case=False, \n",
    "    train_batch_size=8, #8개의 example씩 모델에서 학습. 학습 시간을 줄이는 효과가 있음. \n",
    "    eval_batch_size=8, \n",
    "    learning_rate=5e-5, \n",
    "    num_train_epochs=1, \n",
    "    warmup_proportion=0.3, \n",
    "    no_cuda=False, #cuda를 사용함. \n",
    "    local_rank=-1, \n",
    "    seed=123, \n",
    "    gradient_accumulation_steps=1, \n",
    "    fp16=False, #빠르게 계산하는 기능 사용하지 않음. \n",
    "    loss_scale=0\n",
    "    ):\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args(\"\")\n",
    "    \n",
    "    ## Required parameters\n",
    "    args.case = case\n",
    "    args.data_dirs = data_dirs\n",
    "    args.bert_model_type = bert_model_type # 모델 타입\n",
    "    args.bert_models = bert_models # 모델 경로\n",
    "    args.task_name = task_name  # Glue task name\n",
    "    args.output_dirs = output_dirs # 출력경로\n",
    "    args.max_seq_length = max_seq_length \n",
    "    args.do_train = do_train\n",
    "    args.do_eval = do_eval\n",
    "    args.do_test = do_test\n",
    "    args.do_lower_case = do_lower_case\n",
    "    args.train_batch_size = train_batch_size\n",
    "    args.eval_batch_size = eval_batch_size\n",
    "    args.learning_rate = learning_rate\n",
    "    args.num_train_epochs = num_train_epochs\n",
    "    args.warmup_proportion = num_train_epochs\n",
    "    args.no_cuda = no_cuda\n",
    "    args.local_rank = local_rank\n",
    "    args.seed = seed\n",
    "    args.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "    args.fp16 = fp16\n",
    "    args.loss_scale = loss_scale\n",
    "    \n",
    "\n",
    "    processors = {\n",
    "#         \"cola\": ColaProcessor,\n",
    "#         \"mnli\": MnliProcessor,\n",
    "        \"thy\": ThyProcessor,\n",
    "    }\n",
    "\n",
    "    num_labels_task = {\n",
    "        \"cola\": 2,\n",
    "        \"mnli\": 3,\n",
    "        \"mrpc\": 2, #mrpc의 클래스는 2개(0, 1)이다.  \n",
    "        \"thy\": 3,\n",
    "    }\n",
    "    \"\"\"local_rank, no_cuda argument에 대하여 - gpu 설정\"\"\"\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if not args.do_train and not args.do_eval and not args.do_test:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` or `do_test` must be True.\")\n",
    "\n",
    "#     for d in args.output_dirs: \n",
    "#         if os.path.exists(d) and os.listdir(d):\n",
    "#             raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(d))\n",
    "#         os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    task_name = args.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    num_labels = num_labels_task[task_name]\n",
    "    label_list = processor.get_labels() # 레이블의 종류가 들어감\n",
    "\n",
    "#     print(\"main():: num_labels:\", num_labels)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model_type, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    if args.do_train:\n",
    "        do_train_func(args, device, processor, num_labels, label_list, tokenizer, n_gpu)\n",
    "    if args.do_test:\n",
    "        do_test_func(args, device, processor, num_labels, label_list, tokenizer, n_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_list = [\"../data/uneven-even/\",\"../data/even-even/\",\n",
    "                \"../data/upeven-even/\"]\n",
    "bert_model_type = \"bert-base-uncased\" # 모델 타입을 지정\n",
    "model_dir_list = [\"./bert_result/uneven-even/model/\",\"./bert_result/even-even/model/\",\n",
    "                  \"./bert_result/upeven-even/model/\"]\n",
    "output_dir_list =  [\"./bert_result/uneven-even/\",\"./bert_result/even-even/\",\n",
    "                    \"./bert_result/upeven-even/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/23/2020 12:18:26 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/23/2020 12:18:28 - INFO - __main__ -   LOOKING AT ../data/uneven-even/train.tsv\n",
      "11/23/2020 12:18:29 - INFO - filelock -   Lock 140276203646096 acquired on /home/yoonjin/.cache/torch/transformers/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e06ced3c7614764ba84f38c7d824dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/23/2020 12:19:33 - INFO - filelock -   Lock 140276203646096 released on /home/yoonjin/.cache/torch/transformers/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/23/2020 12:19:39 - INFO - __main__ -   ***** Running training *****\n",
      "11/23/2020 12:19:39 - INFO - __main__ -     Num examples = 1196\n",
      "11/23/2020 12:19:39 - INFO - __main__ -     Batch size = 8\n",
      "11/23/2020 12:19:39 - INFO - __main__ -     Num steps = 1495\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3300, 2365, 2080, 2561, 29610, 22471, 16940, 2110, 100, 1012, 3300, 100, 3742, 100, 100, 100, 100, 1048, 24335, 8458, 13045, 100, 1012, 2187, 16492, 25339, 100, 100, 1048, 24335, 8458, 13045, 100, 100, 1012, 100, 100, 2053, 2695, 7361, 1012, 5866, 4531, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.79 GiB total capacity; 6.41 GiB already allocated; 13.88 MiB free; 6.45 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-64a1266edd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdo_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdo_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdo_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     do_train=False,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c26f722ef03a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(case, data_dirs, bert_model_type, bert_models, task_name, output_dirs, max_seq_length, do_train, do_eval, do_test, do_lower_case, train_batch_size, eval_batch_size, learning_rate, num_train_epochs, warmup_proportion, no_cuda, local_rank, seed, gradient_accumulation_steps, fp16, loss_scale)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdo_train_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mdo_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5b90de119c27>\u001b[0m in \u001b[0;36mdo_train_func\u001b[0;34m(args, device, processor, num_labels, label_list, tokenizer, n_gpu)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# loss 의 1: logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             loss = model(input_ids=input_ids, token_type_ids=segment_ids, \n\u001b[0;32m--> 128\u001b[0;31m                          attention_mask=input_mask, labels=label_ids)\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;31m#print(\"logits: \", loss[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         )\n\u001b[1;32m    850\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 )\n\u001b[1;32m    485\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 170\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/medinfo2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 layer_norm, (input,), input, normalized_shape, weight=weight, bias=bias, eps=eps)\n\u001b[1;32m   2094\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 2095\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.79 GiB total capacity; 6.41 GiB already allocated; 13.88 MiB free; 6.45 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\n",
    "#training with uneven - uneven \n",
    "main(\n",
    "    case = 0,#0: uneven-even/ 1: even-even/ 2: upeven-even  \n",
    "    data_dirs=data_dir_list, # 입력데이터 경로\n",
    "#     data_dir=data_dir_test[0], # 입력데이터 경로\n",
    "    bert_model_type=bert_model_type, # 돌리는 모델의 타입\n",
    "    bert_models=model_dir_list, # 모델 경로. 실제모델과 모델의 설정(json)을 같은 폴더에 넣고 돌려야 함\n",
    "    \n",
    "    #config.json, pytorch_model.bin, vocab.txt 3가지 파일이 있어야 한다. \n",
    "    #이때 파일 이름과 bert_model의 경로 설정에 주의해야 한다. \n",
    "    task_name=\"THY\", # Glue task name\n",
    "    output_dirs=output_dir_list, # 출력경로\n",
    "    \n",
    "    learning_rate=5e-3, \n",
    "    num_train_epochs=100,\n",
    "    warmup_proportion=0.1,\n",
    "    #warmup proportion: training에서 주는 것 중요. 예를 들어, 총 epoch이 1000일 때,\n",
    "    #warmup_proportion이 0.1이면 초반 1000*0.1=100의 epoch동안 lr/100*k k가 1~100까지 늘어나면서 계산됨. \n",
    "    \n",
    "    train_batch_size=8, \n",
    "    eval_batch_size=8, \n",
    "\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    do_test=False, \n",
    "\n",
    "#     do_train=False, \n",
    "#     do_eval=True,\n",
    "#     do_test=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with uneven - uneven \n",
    "main(\n",
    "    case = 1,#0: uneven-even/ 1: even-even/ 2: upeven-even  \n",
    "    data_dirs=data_dir_list, # 입력데이터 경로\n",
    "#     data_dir=data_dir_test[0], # 입력데이터 경로\n",
    "    bert_model_type=bert_model_type, # 돌리는 모델의 타입\n",
    "    bert_models=model_dir_list, # 모델 경로. 실제모델과 모델의 설정(json)을 같은 폴더에 넣고 돌려야 함\n",
    "    \n",
    "    #config.json, pytorch_model.bin, vocab.txt 3가지 파일이 있어야 한다. \n",
    "    #이때 파일 이름과 bert_model의 경로 설정에 주의해야 한다. \n",
    "    task_name=\"THY\", # Glue task name\n",
    "    output_dirs=output_dir_list, # 출력경로\n",
    "    \n",
    "    learning_rate=1e-1,\n",
    "    num_train_epochs=10,\n",
    "    warmup_proportion=0.0,\n",
    "    \n",
    "    train_batch_size=8, \n",
    "    eval_batch_size=8, \n",
    "\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    do_test=False, \n",
    "\n",
    "#     do_train=False, \n",
    "#     do_eval=True,\n",
    "#     do_test=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with uneven - uneven \n",
    "main(\n",
    "    case = 2,#0: uneven-even/ 1: even-even/ 2: upeven-even  \n",
    "    data_dirs=data_dir_list, # 입력데이터 경로\n",
    "#     data_dir=data_dir_test[0], # 입력데이터 경로\n",
    "    bert_model_type=bert_model_type, # 돌리는 모델의 타입\n",
    "    bert_models=model_dir_list, # 모델 경로. 실제모델과 모델의 설정(json)을 같은 폴더에 넣고 돌려야 함\n",
    "    \n",
    "    #config.json, pytorch_model.bin, vocab.txt 3가지 파일이 있어야 한다. \n",
    "    #이때 파일 이름과 bert_model의 경로 설정에 주의해야 한다. \n",
    "    task_name=\"THY\", # Glue task name\n",
    "    output_dirs=output_dir_list, # 출력경로\n",
    "    \n",
    "    learning_rate=1e-1,\n",
    "    num_train_epochs=10,\n",
    "    warmup_proportion=0.0,\n",
    "    \n",
    "    train_batch_size=8, \n",
    "    eval_batch_size=8, \n",
    "\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    do_test=False, \n",
    "\n",
    "#     do_train=False, \n",
    "#     do_eval=True,\n",
    "#     do_test=False, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinfo2",
   "language": "python",
   "name": "medinfo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
